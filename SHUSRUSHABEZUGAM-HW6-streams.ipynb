{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5\n",
    "\n",
    "Copy this notebook. Rename it as: YOURNAME-HW5-streams \n",
    "\n",
    "with your name replacing YOURNAME.\n",
    "\n",
    "Upload your completed jupyter notebook to elearning site as your homework submission. You can put this notebook on your github.\n",
    "\n",
    "5.1  Register for a stream of Twitter data\n",
    "\n",
    "5.2  Create a bloom filter classifying two days worth of twitters  ( after removing stop words and urls )\n",
    "\n",
    "5.3  For another days worth of twitter data find the previous twitters that match in the bloom filter\n",
    "(This means get two days of data in one file or directory , use that data for training the bloom filter, capture a different days data in a different file ( or do it in real time)and capture the match output then running the new twitter data through the filter.\n",
    "\n",
    "5.4 Plot a historgram of matches for each twitter in 5.3\n",
    "\n",
    "For the 4-5 grade.- Submit in a separate notebook - YourNAME-Homework5-Supplement\n",
    "\n",
    "1. Use a different machine learning training algorithm\n",
    "2. Make a continous feed where you take two days of data and match the incoming stream ( do this for 5 days windowing the filter data)\n",
    "3. Find new trends in the twitter feed (daily or hourly)\n",
    "4. Or some other streaming exploration of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting twitter_streamingSB.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile twitter_streamingSB.py\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"925146948012822528-BixZTf8fAkM4s5j89b7wT8rDm1FVALs\"\n",
    "access_token_secret = \"EjsuYJuhHQK0WnIkxnIrl8qQXCZx6cgHd1yygsejn7njU\"\n",
    "consumer_key = \"1PUalMqq9WpgZ5o6bJUtPKJNJ\"\n",
    "consumer_secret = \"U1N73shHsc27QFLNRCi0u9KjayMlupMSQc8nCA1ZZEAV5UMqQm\"\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print data\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'apple', 'google', 'microsoft'\n",
    "    stream.filter(track=['apple', 'google', 'microsoft'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.15063]\r\n",
      "(c) 2017 Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "(C:\\Users\\AsusGo\\Anaconda2) C:\\Users\\AsusGo\\Downloads\\big-data-python-class-master\\Homeworks\\Homework6>python twitter_streamingSB.py > twitter_data_pat3.txt\n",
      "\r\n",
      "(C:\\Users\\AsusGo\\Anaconda2) C:\\Users\\AsusGo\\Downloads\\big-data-python-class-master\\Homeworks\\Homework6>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"twitter_streamingSB.py\", line 31, in <module>\r\n",
      "    stream.filter(track=['apple', 'google', 'microsoft'])\r\n",
      "  File \"C:\\Users\\AsusGo\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.py\", line 445, in filter\r\n",
      "    self._start(async)\r\n",
      "  File \"C:\\Users\\AsusGo\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.py\", line 361, in _start\r\n",
      "    self._run()\r\n",
      "  File \"C:\\Users\\AsusGo\\Anaconda2\\lib\\site-packages\\tweepy\\streaming.py\", line 294, in _run\r\n",
      "    raise exception\r\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='stream.twitter.com', port=443): Read timed out.\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "python twitter_streamingSB.py > twitter_data_pat2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pier 32 Industry: Magikarp ‚ôÇ til 17:33:00(13m ...</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/SCk9CG19sy\\n\\n#DavyJones</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @M_Madoka_com: „ÄäGoogle+Êõ¥Êñ∞„Äã\\n11/12\\n\\nÊù±‰∫¨ÁùÄ„Åç„Åæ„Åó...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Send money to his opponent @GDouglasJones - Tr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when google celebrates his birthday!!\\n‡¶∂‡ßÅ‡¶≠ ‡¶ú‡¶®‡ßç...</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Riachuelo? Prefiro andrajo como indument√°ria.\\...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @CNET: Microsoft CEO tells iPad users to 'g...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&amp;gt;&amp;gt; https://t.co/LWgbwOPous</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dog Beach: Magikarp ‚ôÇ til 17:49:17(29m 49s). h...</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ÿ≠ŸÖŸëŸÑ ÿßŸÑÿ¢ŸÜ ŸÑÿπÿ®ÿ© #ŸÉŸÑŸÖÿ©ŸÄÿßŸÑÿ≥ÿ± #ÿßŸÑÿ¨ÿ≤ÿ°ŸÄŸ¢ #ÿ≤Ÿäÿ™ŸàŸÜÿ© ÿ£ŸäŸÅ...</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang\n",
       "0  Pier 32 Industry: Magikarp ‚ôÇ til 17:33:00(13m ...   et\n",
       "1              https://t.co/SCk9CG19sy\\n\\n#DavyJones  und\n",
       "2  RT @M_Madoka_com: „ÄäGoogle+Êõ¥Êñ∞„Äã\\n11/12\\n\\nÊù±‰∫¨ÁùÄ„Åç„Åæ„Åó...   ja\n",
       "3  Send money to his opponent @GDouglasJones - Tr...   en\n",
       "4  when google celebrates his birthday!!\\n‡¶∂‡ßÅ‡¶≠ ‡¶ú‡¶®‡ßç...  und\n",
       "5  Riachuelo? Prefiro andrajo como indument√°ria.\\...   pt\n",
       "6  RT @CNET: Microsoft CEO tells iPad users to 'g...   en\n",
       "7                   &gt;&gt; https://t.co/LWgbwOPous  und\n",
       "8  Dog Beach: Magikarp ‚ôÇ til 17:49:17(29m 49s). h...   da\n",
       "9  ÿ≠ŸÖŸëŸÑ ÿßŸÑÿ¢ŸÜ ŸÑÿπÿ®ÿ© #ŸÉŸÑŸÖÿ©ŸÄÿßŸÑÿ≥ÿ± #ÿßŸÑÿ¨ÿ≤ÿ°ŸÄŸ¢ #ÿ≤Ÿäÿ™ŸàŸÜÿ© ÿ£ŸäŸÅ...   ar"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "tweets_data_path = 'twitter_data_part2.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data)\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = [tweet.get('text','') for tweet in tweets_data]\n",
    "tweets['lang'] = [tweet.get('lang','') for tweet in tweets_data]\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     Send money to his opponent @GDouglasJones - Tr...\n",
      "6     RT @CNET: Microsoft CEO tells iPad users to 'g...\n",
      "12    RT @KJKirby: The latest Microsoft Project Dail...\n",
      "14    RT @vadpradub: #JudgeMoore accuser should be i...\n",
      "15    Target: Kellogg‚Äôs Pop-Tarts Caramel Apple or P...\n",
      "20    RT @realtycoinsinc: Notable and Influential An...\n",
      "21    RT @seattlewebseo: Google Grants will help you...\n",
      "22    RT @JoeFreedomLove: Here's how the House and S...\n",
      "25    They called it \"Dox?\"\\n\\nThey. Called. It. Dox...\n",
      "26    RT @JonathanYMusic: üêâ SKYRIM THEME ~ METAL CO...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tweets.groupby(['lang']=='en')['text']\n",
    "tweet_text=tweets.loc[tweets['lang'] == 'en', 'text']\n",
    "\n",
    "print tweet_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I filtered the english tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimination of stop words,special characters,urls and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     Send money to his opponent DouglasJones  Trump...\n",
      "6                                                      \n",
      "12                                               espc17\n",
      "14                                                     \n",
      "15                                Target Kelloggs PopTa\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          85552\n",
       "the        5300\n",
       "to         5192\n",
       "Google     4655\n",
       "a          4039\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_without_urls = tweet_text.str.replace('http\\S+|www.\\S', ' ', case=False)\n",
    "tweet_clean_data=tweet_text_without_urls.str.replace('@[A-Za-z0-9]|[^0-9A-Za-z \\t]|RT+.*'+'[^A-Za-z0-9]+','', case=False)\n",
    "#tweet_text_wo_urls_SpecChar=tweet_text_without_urls.str.replace('[^A-Za-z0-9]+','  ',case=False)\n",
    "#tweet_text_wo_urls_SpecChar_RT=tweet_text_wo_urls_SpecChar.str.replace('RT+.*\\:','', case=False)\n",
    "#(@[A-Za-z0-9]+)+([^0-9A-Za-z \\t])+(\\w+:\\/\\/\\S+)\n",
    "print tweet_clean_data.head(5)\n",
    "\n",
    "#tweet_text_wo_urls_stopwords_RT_SpecChar=tweet_text_wo_urls_stopwords_RT.str.replace('[^A-Za-z0-9]+','', case=False)\n",
    "tweet_clean_data.str.split(' ', expand=True).stack().unique()\n",
    "tweet_clean_data.str.split(' ', expand=True).stack().value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,I removed the url links from all the rows by using replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will elminate the stop words from the sub dataframe with the help of nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AsusGo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Send' u'money' u'opponent' ..., u'20132529m' u'19000' u'081416pm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "tweet_clean_data_without_stopwords=tweet_clean_data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#print tweet_clean_data_without_stopwords\n",
    "#print tweet_text_without_urls_stopwords\n",
    "#tweet_clean_data_without_stopwords.split(' ', expand=True).stack().unique()\n",
    "#tweet_clean_data_without_stopwords.str.split(' ', expand=True).stack().value_counts().head()\n",
    "new_tweets_clean_data=tweet_clean_data_without_stopwords.str.split(' ', expand=True).stack().unique()\n",
    "tweet_clean_data_without_stopwords.str.split(' ', expand=True).stack().value_counts()\n",
    "print new_tweets_clean_data\n",
    "#result = ' '.join(tweet_text_without_urls_stopwords)\n",
    "#print result.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I elminated the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#new_tweets_clean_data.to_csv(r'c:\\data\\twitter_text_ouput1.txt', header=None, index=None, sep=' ', mode='a')\n",
    "#np.savetxt('test.out', new_tweets_clean_data, delimiter=' ') \n",
    "np.savetxt(r'data/np.txt', new_tweets_clean_data, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the data of ndarray to a txt file using savetxt function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "import os\n",
    "import re\n",
    "\n",
    "POST_DIR = 'data/'\n",
    "# Read all my posts.\n",
    "posts = {post_name: open(POST_DIR + post_name).read() for post_name in os.listdir(POST_DIR)}\n",
    "# Create a dictionary of {\"post name\": \"lowercase word set\"}.\n",
    "split_posts = {name: set(re.split(\"\\W+\", contents.lower())) for name, contents in posts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = {}\n",
    "for name, words in split_posts.items():\n",
    "    filters[name] = BloomFilter(capacity=len(words), error_rate=0.1)\n",
    "    for word in words:\n",
    "        filters[name].add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "References:\n",
    "\n",
    "http://localhost:8890/notebooks/Downloads/big-data-python-class-master/Lectures/Lecture6-Streams/Streams%20Tutorial.ipynb\n",
    "\n",
    "https://stackoverflow.com/questions/45395676/remove-a-url-row-by-row-from-a-large-set-of-text-in-python-panda-dataframe\n",
    "\n",
    "https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "\n",
    "https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\n",
    "\n",
    "https://stackoverflow.com/questions/32425334/splitting-a-string-in-a-python-dataframe\n",
    "\n",
    "https://stackoverflow.com/questions/26594817/remove-and-rt-from-the-tweet\n",
    "\n",
    "https://stackoverflow.com/questions/38557617/how-to-get-all-the-unique-words-in-the-data-frame\n",
    "\n",
    "https://stackoverflow.com/questions/31247198/python-pandas-write-content-of-dataframe-into-text-file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
